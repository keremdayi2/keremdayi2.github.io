<!doctype html><html lang=en style=font-size:120%><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content><meta name=description content="We rigorously analyze SGD on learning rank-1 perturbations of two-layer neural networks beyond the kernel regime. We show that fine-tuning sits between the kernel-regime and the feature learning regime, and we can prove complexity rigorous separations between fine-tuning and 'learning from scratch'"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><meta name=keywords content="hugo latex theme blog texify texify2 weastur"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script type=module>
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script><meta property="og:url" content="https://keremdayi2.github.io/research/gradient-dynamics/"><meta property="og:site_name" content="Arif Kerem Dayi"><meta property="og:title" content="Gradient Dynamics for low-rank fine-tuning"><meta property="og:description" content="We rigorously analyze SGD on learning rank-1 perturbations of two-layer neural networks beyond the kernel regime. We show that fine-tuning sits between the kernel-regime and the feature learning regime, and we can prove complexity rigorous separations between fine-tuning and 'learning from scratch'"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="research"><meta property="article:published_time" content="2024-12-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-01T22:02:26-05:00"><meta property="og:image" content="https://keremdayi2.github.io/images/logo.jpg"><link rel=canonical href=https://keremdayi2.github.io/research/gradient-dynamics/><meta itemprop=name content="Gradient Dynamics for low-rank fine-tuning"><meta itemprop=description content="We rigorously analyze SGD on learning rank-1 perturbations of two-layer neural networks beyond the kernel regime. We show that fine-tuning sits between the kernel-regime and the feature learning regime, and we can prove complexity rigorous separations between fine-tuning and 'learning from scratch'"><meta itemprop=datePublished content="2024-12-01T00:00:00+00:00"><meta itemprop=dateModified content="2024-12-01T22:02:26-05:00"><meta itemprop=image content="https://keremdayi2.github.io/images/logo.jpg"><link media=screen rel=stylesheet href=https://keremdayi2.github.io/css/common.css><link media=screen rel=stylesheet href=https://keremdayi2.github.io/css/content.css><title>Gradient Dynamics for low-rank fine-tuning - </title><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://keremdayi2.github.io/images/logo.jpg"><meta name=twitter:title content="Gradient Dynamics for low-rank fine-tuning"><meta name=twitter:description content="We rigorously analyze SGD on learning rank-1 perturbations of two-layer neural networks beyond the kernel regime. We show that fine-tuning sits between the kernel-regime and the feature learning regime, and we can prove complexity rigorous separations between fine-tuning and 'learning from scratch'"><link rel=stylesheet href=https://keremdayi2.github.io/css/single.css><link rel=stylesheet href=https://keremdayi2.github.io/css/sharingbuttons.css></head><body><div id=wrapper><header id=header><h1><a href=https://keremdayi2.github.io/>Arif Kerem Dayi</a></h1><nav><span class=nav-bar-item><a class=link href=/>Home</a>
</span><span class=nav-bar-item><a class=link href=/research/>Research</a></span></nav></header><main id=main class=post><article class="content numbered-subtitles"></article></main><footer id=footer><div><span>© 2023</span> - <span>2024</span></div><div><span>Powered by </span><a class=link target=_blank href=https://gohugo.io/>Hugo</a>
<span>🍦 Theme </span><a class=link target=_blank href=https://texify2.io>TeXify2</a></div><div class=footnote><span>Follow me on <a class=link href=https://github.com/weastur target=_blank rel=noopener>GitHub</a>,
<a class=link href=https://www.linkedin.com/in/weastur/ target=_blank rel=noopener>LinkedIn</a> or
<a class=link href=/index.xml>RSS</a> |
<a class=link href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en target=_blank rel=noopener>CC BY-NC-SA 4.0</a></span></div></footer></div></body></html>